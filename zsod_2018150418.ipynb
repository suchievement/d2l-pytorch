{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suchievement/d2l-pytorch/blob/master/zsod_2018150418.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation + Import\n"
      ],
      "metadata": {
        "id": "mMIj740_YhEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7RQZQhOLyAB",
        "outputId": "5cfaed87-faab-49de-949b-cd1fc80ffad2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-yssnh32y\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-yssnh32y\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 587 kB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (4.64.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from clip==1.0) (0.13.1+cu113)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->clip==1.0) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision->clip==1.0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision->clip==1.0) (1.21.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision->clip==1.0) (3.0.4)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369408 sha256=89af17fe5d30d211849cdf3817f5841e355f7f34061a02917a0888c1fd7c2488\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ai1y20be/wheels/ab/4f/3a/5e51521b55997aa6f0690e095c08824219753128ce8d9969a3\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selectivesearch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEjQ8AiSLzGM",
        "outputId": "0617d61f-0212-4bed-9250-1a8a1d452361"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting selectivesearch\n",
            "  Downloading selectivesearch-0.4.tar.gz (3.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from selectivesearch) (1.21.6)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.8/dist-packages (from selectivesearch) (0.18.3)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image->selectivesearch) (7.1.2)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image->selectivesearch) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image->selectivesearch) (1.3.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image->selectivesearch) (2.6.3)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image->selectivesearch) (3.2.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image->selectivesearch) (2.9.0)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image->selectivesearch) (1.7.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->selectivesearch) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->selectivesearch) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->selectivesearch) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->selectivesearch) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->selectivesearch) (1.15.0)\n",
            "Building wheels for collected packages: selectivesearch\n",
            "  Building wheel for selectivesearch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for selectivesearch: filename=selectivesearch-0.4-py3-none-any.whl size=4350 sha256=26a6ccbd4c20fb2c39cfb8d22bb0e521e50563196c8736caa7a9e7f8d7b14f9e\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/4e/88/6de23ce74be839a953498c4ebdfa809ad7da9422ac89ae856c\n",
            "Successfully built selectivesearch\n",
            "Installing collected packages: selectivesearch\n",
            "Successfully installed selectivesearch-0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install \"git+https://github.com/philferriere/cocoapi.git#egg=pycocotools&subdirectory=PythonAPI\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTuITEXbL097",
        "outputId": "dbc07d55-e442-4d8f-cad4-8c3f5ae89573"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pycocotools\n",
            "  Cloning https://github.com/philferriere/cocoapi.git to /tmp/pip-install-wasa6exq/pycocotools_132df2b7c1f744e8b156f2644c0f9124\n",
            "  Running command git clone -q https://github.com/philferriere/cocoapi.git /tmp/pip-install-wasa6exq/pycocotools_132df2b7c1f744e8b156f2644c0f9124\n",
            "Building wheels for collected packages: pycocotools\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0-cp38-cp38-linux_x86_64.whl size=304781 sha256=e9158b3e580ea5177f005d78171ff1c18e5107d4bfe77892cbb54b522795b727\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-741_bk6o/wheels/bd/1c/0d/8c82e1b9bc855b82e1eb53eadea4459efe171d2daf5a222701\n",
            "Successfully built pycocotools\n",
            "Installing collected packages: pycocotools\n",
            "  Attempting uninstall: pycocotools\n",
            "    Found existing installation: pycocotools 2.0.6\n",
            "    Uninstalling pycocotools-2.0.6:\n",
            "      Successfully uninstalled pycocotools-2.0.6\n",
            "Successfully installed pycocotools-2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AloXdw-xLUYi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import os\n",
        "import clip\n",
        "from selectivesearch import selective_search\n",
        "import cv2\n",
        "from PIL import Image\n",
        "#import fiftyone as fo\n",
        "from pycocotools.coco import COCO\n",
        "import os.path\n",
        "from typing import Any, Callable, Optional, Tuple, List\n",
        "import json\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byshfDMdLU4H",
        "outputId": "f16fd15d-c800-4a5e-a6ee-0d2671012f32"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/22_2_DL/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTE5F-JOYv_w",
        "outputId": "ab272e34-e90a-43db-abaa-34b44d0de201"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive/22_2_DL/data'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clip\n",
        "model, preprocess = clip.load(\"ViT-L/14\", device=device)"
      ],
      "metadata": {
        "id": "U4YLvuPqqCxR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aeaf594-5fd7-443a-d434-7b44f058cf52"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 890M/890M [00:18<00:00, 51.7MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COCO Dataset 준비"
      ],
      "metadata": {
        "id": "1lBfoiIDY_ij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = \"/content/drive/MyDrive/22_2_DL/data/\"\n",
        "data_path = \"/content/drive/MyDrive/22_2_DL/data/val2017/\"\n",
        "labels_path = \"/content/drive/MyDrive/22_2_DL/data/annotations/instances_val2017.json\""
      ],
      "metadata": {
        "id": "tOZivXUTMjJc"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1F8oECZT4oEL",
        "outputId": "1646d8c8-a70a-41f1-928e-be0fbc518207"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "coco = COCO(labels_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpitIm_2ZKSb",
        "outputId": "ac346d78-809a-424c-fcb2-d338f704526f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.73s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "coco.loadImgs(285)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBPpPYIHZRGJ",
        "outputId": "150ee694-3b8a-4ca5-adb5-503defff411f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'license': 4,\n",
              "  'file_name': '000000000285.jpg',\n",
              "  'coco_url': 'http://images.cocodataset.org/val2017/000000000285.jpg',\n",
              "  'height': 640,\n",
              "  'width': 586,\n",
              "  'date_captured': '2013-11-18 13:09:47',\n",
              "  'flickr_url': 'http://farm8.staticflickr.com/7434/9138147604_c6225224b8_z.jpg',\n",
              "  'id': 285}]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0 CLIP의 텍스트 인코더로 각 class에 대한 임베딩 생성\n"
      ],
      "metadata": {
        "id": "Ku0ZG81X0Rvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pycocotools를 이용해 names를 리스트에 저장\n",
        "names=[]\n",
        "cats = coco.loadCats(coco.getCatIds())\n",
        "names.append([cat['name'] for cat in cats])\n",
        "names = names[0]"
      ],
      "metadata": {
        "id": "Q73O40ua0dVo"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 80개 category에 대한 텍스트 임베딩을 구함\n",
        "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in names]).to(device)\n",
        "text_features = model.encode_text(text_inputs)\n",
        "text_features = text_features.float()"
      ],
      "metadata": {
        "id": "AaP7ym6c0dVo"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 80개의 768차원 벡터 생성\n",
        "text_features.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a06ad29e-e20c-4a52-9274-e9e7c0e10db0",
        "id": "JE1SoO0y0dVo"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([80, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1 ss의 output 좌표를 coco형식으로 바꾸는 함수\n",
        "- ss,COCO의 output은 (left, top, w, h)\n",
        "- PIL은 (left, top, right, bottom)\n",
        "  - ss[0],ss[1],ss[0]+ss[2],ss[1]+ss[3] 으로 바꾸기\n"
      ],
      "metadata": {
        "id": "xKT_WgjQOFVq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ss에서 나온 좌표를 bboxes라는 리스트에 담아 return하는 함수\n",
        "def stc(selectivesearch_output):\n",
        "  bboxes = []\n",
        "  for ss in selectivesearch_output:\n",
        "    bboxes.append([ss['rect'][0],ss['rect'][1],ss['rect'][0]+ss['rect'][2],ss['rect'][1]+ss['rect'][3]])\n",
        "  return bboxes"
      ],
      "metadata": {
        "id": "pKIrLHdgL0c6"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. gt를 바꿔주는 함수\n",
        "def stc2(selectivesearch_output):\n",
        "  bboxes = []\n",
        "  for ss in selectivesearch_output:\n",
        "    bboxes.append([ss[0],ss[1],ss[0]+ss[2],ss[1]+ss[3]])\n",
        "  return bboxes"
      ],
      "metadata": {
        "id": "dzRdw86w6GLu"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2 하나의 이미지 path를 받아 ROI 출력하는 함수"
      ],
      "metadata": {
        "id": "FLAMZpE1Yoxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. 하나의 이미지 path를 받아 모든 ROI의 768차원 임베딩을 생성하는 함수\n",
        "# 3까지 같이해보기\n",
        "## scale은 알고리즘이 찾는 ROI의 최소크기 \n",
        "## min_size는 그 중 선택할 ROI의 최소크기 \n",
        "## \n",
        "def ROI(image_filename, scale=2000, min_size=2000):\n",
        "  img = Image.open(data_path+image_filename)\n",
        "  img_np = np.array(img)\n",
        "  _, regions = selective_search(img_np, scale=scale, min_size=min_size)\n",
        "  bboxes = stc(regions)\n",
        "\n",
        "  img_features = torch.empty(1,768).to(device)\n",
        "  roi_image = []\n",
        "  for bbox in bboxes:\n",
        "    roi = img.crop((bbox[0],bbox[1],bbox[2],bbox[3]))\n",
        "    roi_image.append(roi)\n",
        "    img_input = (preprocess(roi).unsqueeze(0).to(device))\n",
        "    with torch.no_grad():\n",
        "     img_feature = model.encode_image(img_input)\n",
        "     img_features = torch.cat((img_features, img_feature))\n",
        "  img_features = img_features[1:,]\n",
        "  return img_features, roi_image, bboxes"
      ],
      "metadata": {
        "id": "tAjWF5NZZQ5a"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 모든 ROI 와 각 category 사이의 코사인 유사도 구하기\n",
        "## text features는 이미 normalized됐다고 가정\n",
        "\n"
      ],
      "metadata": {
        "id": "20VWNj3Qk867"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bbox, gt, cls를 리턴하는 함수\n",
        "## 모든 ROI 중 제일 비슷한 cls와의 유사도가 threshold를 넘는 roi 빼고 suppress.\n",
        "## 그렇게 구한 BBOX 별로 gt와 IOU를 계산해 일정 최대 IOU를 갖는 GT와 Match\n",
        "## 이미지에 있는 gt를 각자의 cls와 함께 저장한다.\n",
        "def scores(features):\n",
        "  features /= features.norm(dim=-1, keepdim=True)\n",
        "  text_features_norm = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "  sim = torch.mm(features, text_features.T)\n",
        "  # 각 클래스 텍스트 임베딩과의 cos sim\n",
        "  probs = sim.softmax(dim=1)\n",
        "  # 각 bbox별로 cls할당\n",
        "  values, idx_names = torch.max(probs, dim=1)\n",
        "  \n",
        "  return idx_names, values"
      ],
      "metadata": {
        "id": "xr4dCHmWRcj2"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NMS를 위해 bboxes, values, idx_names를 values를 기준으로 sort해주는 함수\n"
      ],
      "metadata": {
        "id": "L3PMGsakiMwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sort_for_nms(bboxes, values, idx_names):\n",
        "  sort_idx = sorted(range(len(values)), key=lambda k: values[k], reverse=True)\n",
        "  bboxes_output = torch.tensor(bboxes)[sort_idx]\n",
        "  values_output = values[sort_idx]\n",
        "  idx_names_output = idx_names[sort_idx]\n",
        "  return bboxes_output, values_output, idx_names_output"
      ],
      "metadata": {
        "id": "jZ_lZRXaiS1w"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. NMS\n",
        "1. NMS \n",
        "- Scores 함수에서 max(probs)(가장 비슷한 cls와 유사도)가 0.04 이상인 box만 남기기\n",
        "- 이후 남은 박스들에 대해 NMS진행해 또 거르기\n",
        "\n",
        "  --> 남은 박스의 인덱스를 활용해 ROI함수의 output인 bboxes에서 해당 박스들의 좌표를 가져와 keep\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wmA1j8BptgMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IOU 구하는 함수 \n",
        "def IoU(box1, box2):\n",
        "    # box = (x1, y1, x2, y2)\n",
        "    box1_area = (box1[2] - box1[0] + 1) * (box1[3] - box1[1] + 1)\n",
        "    box2_area = (box2[2] - box2[0] + 1) * (box2[3] - box2[1] + 1)\n",
        "\n",
        "    # obtain x1, y1, x2, y2 of the intersection\n",
        "    x1 = max(box1[0], box2[0])\n",
        "    y1 = max(box1[1], box2[1])\n",
        "    x2 = min(box1[2], box2[2])\n",
        "    y2 = min(box1[3], box2[3])\n",
        "\n",
        "    # compute the width and height of the intersection\n",
        "    w = max(0, x2 - x1 + 1)\n",
        "    h = max(0, y2 - y1 + 1)\n",
        "\n",
        "    inter = w * h\n",
        "    iou = inter / (box1_area + box2_area - inter+1e-7)\n",
        "    return iou"
      ],
      "metadata": {
        "id": "LpjLMT7GUhqC"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NMS 해주는 함수\n",
        "def NMS(bboxes, values, idx_names, conf_threshold=0.03, iou_threshold=0.5):\n",
        "  values = values.cpu()\n",
        "  # confidence가 낮은(기본값 0.04) bbox는 제거\n",
        "  idx = np.where((values>=conf_threshold))[0]\n",
        "  # NMS\n",
        "  bbs_filtered=np.array(bboxes)[idx]\n",
        "  values_filtered = values.detach().numpy()\n",
        "  idx_names_filtered = idx_names.cpu().numpy()\n",
        "\n",
        "  sup = torch.ones(len(bbs_filtered))\n",
        "  \n",
        "  for i in range(len(bbs_filtered)):\n",
        "    for j in range(1, len(bbs_filtered)):\n",
        "      iou = IoU(bbs_filtered[i],bbs_filtered[j])\n",
        "      if (iou>=iou_threshold) and (idx_names_filtered[i]==idx_names_filtered[j]):\n",
        "        sup[j] = 0\n",
        "  sup_idx = np.where(sup)\n",
        "  bbs_sup = bbs_filtered[sup_idx]\n",
        "  values_sup = values_filtered[sup_idx]\n",
        "  idx_names_sup = idx_names_filtered[sup_idx]\n",
        "  return bbs_sup, values_sup, idx_names_sup\n",
        "\n"
      ],
      "metadata": {
        "id": "kpT3nwaTd6uJ"
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Matching, 결과 기록\n",
        "\n",
        "- 하나의 이미지에서 t개의 gt 불러오기\n",
        "- 1번에서 구한 d개의 detection 불러오기\n",
        "\n",
        "- 이미지에서 t개의 gt를 불러오기\n",
        "  - d개의 detection과 t개의 gt를 count_det와 count_gt에 저장\n",
        "- 1에서 구한 bb를 score 내림차순 정렬\n",
        "- 각 bb마다 IOU가 가장 높은 gt를 match\n",
        "  - iou가 threshold 이상\n",
        "    - 여기서 아니면 correct = 0\n",
        "  - 두 box의 cls가 같음\n",
        "    - 여기서 아니면 correct = 0\n",
        "  - 해당 gt에 이미 매칭된 bbox가 없음\n",
        "    - 해당 bbox가 해당하는 correct = 1\n",
        "    - 여기서 아니면 correct = 0\n",
        "  "
      ],
      "metadata": {
        "id": "UqyaDpjii5cI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 이미지 이름은 os.listdir(data_path)[i]로 줄 것\n",
        "def matcher(bboxes_sup, values_sup, idx_names_sup, image_filename ):\n",
        "  img = Image.open(data_path+image_filename)\n",
        "  im_id = int(image_filename[:-4])\n",
        "  # gt 가져오기\n",
        "  gt_list = coco.getAnnIds(imgIds=im_id)\n",
        "  gt_gtbb_nms=[]\n",
        "  gtbbs=[]\n",
        "  for gt in gt_list:\n",
        "    gtbb = coco.loadAnns(gt)[0]['bbox']\n",
        "    nms = coco.loadAnns(gt)[0]['category_id']\n",
        "    gt_gtbb_nms.append([gt, stc2([gtbb])[0], nms])\n",
        "    gtbbs.append(gtbb)\n",
        "\n",
        "\n",
        "  iou=torch.empty(len(gt_gtbb_nms),len(bboxes_sup))\n",
        "  for i,gt in enumerate(gt_gtbb_nms):\n",
        "   for j, bb in enumerate(bboxes_sup):\n",
        "     iou[i,j] = IoU(gt[1], bb)\n",
        "  confs, indices = torch.max(iou, dim=0)\n",
        "  names_bb, confs, indices = sort_for_nms(idx_names_sup, confs,indices)\n",
        "  names_gt = torch.tensor([gt_gtbb_nms[idx][2] for idx in indices])                      \n",
        "  correct = torch.eq(names_bb,names_gt)\n",
        "  return sum(correct)\n",
        "  \n"
      ],
      "metadata": {
        "id": "QJBXF9PGi8Du"
      },
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Result\n",
        "  - 모든 이미지에 대해 test한 결과mAP가 0에 수렴함. (=정답을 맞춘 경우가 거의 없음)\n",
        "  - 처음 20개 결과를 출력해보면 정답의 개수가 매우 적다.\n",
        "    - Image 하나당 정답을 맞춘 경우(IOU >=0.5)가 대부분 0이다."
      ],
      "metadata": {
        "id": "4_8ZojCFt90V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = []\n",
        "for img in os.listdir(data_path)[0:20]:\n",
        "  features, rois, bboxes = ROI(img, scale=2000, min_size=2000)\n",
        "  idx_names, values = scores(features)\n",
        "  bboxes_sorted, values_sorted, idx_names_sorted = sort_for_nms(bboxes, values, idx_names)\n",
        "  bboxes_sup, values_sup, idx_names_sup = NMS(bboxes_sorted, values_sorted, idx_names_sorted, conf_threshold=0.03, iou_threshold=2)\n",
        "  correct = matcher(bboxes_sup, values_sup, idx_names_sup, img)\n",
        "  result.append([img, correct])\n",
        "\n"
      ],
      "metadata": {
        "id": "8-C1o8glxABp"
      },
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "b1FiJVnhusXF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfdce3cc-4f77-4caf-ed18-c3082b2ae286"
      },
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['000000140076.jpg', tensor(0)],\n",
              " ['000000218439.jpg', tensor(0)],\n",
              " ['000000571598.jpg', tensor(0)],\n",
              " ['000000023899.jpg', tensor(0)],\n",
              " ['000000379842.jpg', tensor(0)],\n",
              " ['000000575081.jpg', tensor(0)],\n",
              " ['000000320642.jpg', tensor(0)],\n",
              " ['000000075612.jpg', tensor(0)],\n",
              " ['000000383842.jpg', tensor(0)],\n",
              " ['000000016958.jpg', tensor(1)],\n",
              " ['000000091500.jpg', tensor(0)],\n",
              " ['000000183049.jpg', tensor(0)],\n",
              " ['000000124659.jpg', tensor(0)],\n",
              " ['000000195842.jpg', tensor(0)],\n",
              " ['000000458663.jpg', tensor(0)],\n",
              " ['000000181753.jpg', tensor(0)],\n",
              " ['000000003934.jpg', tensor(0)],\n",
              " ['000000013923.jpg', tensor(1)],\n",
              " ['000000154358.jpg', tensor(0)],\n",
              " ['000000479248.jpg', tensor(0)]]"
            ]
          },
          "metadata": {},
          "execution_count": 248
        }
      ]
    }
  ]
}